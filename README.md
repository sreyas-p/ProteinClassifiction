Integrating Graph Neural Networks with Protein Language Models for Functional Annotation
Abstract
Proteins drive nearly every biological process. Their ability to function depends on their amino acid sequences and the complex ways those sequences fold into three-dimensional structures. Researchers have studied these two features separately for years, either by modeling shapes or analyzing sequence patterns. Recent advances in AI are changing that. Graph neural networks (GNNs) can map protein structures as spatial graphs, whereas protein language models (PLMs), such as ProtBERT, learn from millions of sequences (Brandes et al., 2022; Rives et al., 2021). These methods could be improved and optimized for use in other fields. This paper examines how each method operates, what they offer individually, and how combining them enables a more accurate prediction of protein function.  
Introduction
Automated protein function prediction is a central problem in bioinformatics, driven by the vast sequence databases with millions of entries. For example, while databases like UniProt contain on the order of 200 million protein sequences, only a few million have reliable functional labels.  Traditional homology-based methods often fail to identify remote homologs and novel folds, motivating deep learning approaches that can generalize beyond simple sequence similarity.  In parallel, the advent of protein language models (PLMs) trained on massive sequence corpora has provided powerful sequence representations.  Models such as ProtBERT (Brandes et al., 2022), ESM (Rives et al., 2021), and others learn contextual embeddings of amino acid sequences that capture evolutionary and biophysical patterns (e.g., conserved motifs and domains) even in the absence of labeled data. These embeddings have shown great performance on diverse protein predictions, from secondary structure to subcellular localization.

Beyond DNA sequence, protein function is inherently tied to 3D structure and physical interactions.  Graph neural networks (GNNs), and in particular graph convolutional networks (GCNs), are well-suited to model molecular structures by treating residues as nodes and their physical contacts as edges (Kipf & Welling, 2016)￼. In recent work, Gligorijević et al. demonstrated that a two-stage architecture, using a pre-trained LSTM language model and a multi-layer GCN, significantly outperforms sequence-only or structure-only baselines Gligorijević et al., 2021).  In particular, integrating sequence embeddings and structural graph convolutions resulted in significantly improved function prediction accuracy compared to using contact maps with one-hot encoding (Gligorijević et al., 2021).  These results highlight the complementary nature of sequence and structure information: rich embeddings from PLMs encode long-range evolutionary context, while GCNs capture spatially localized interactions.
In this paper, we extend the hybrid approach by utilizing a modern transformer-based PLM (ProtBERT) and an explicit GCN architecture to determine the optimal method for predicting gene ontology (GO) annotations.
Our contributions include: 
Integrated architecture: We detail a combined model that inputs ProtBERT embeddings of the amino acid sequence and a GCN on the protein’s residue contact graph, merging their outputs for GO-term prediction.
Code-level design: We provide pseudocode and implementation notes for data preprocessing (Protein Data Bank (PDB) parsing, graph construction) and network layers. 
Data pipeline: We propose an automated system for expanding the training dataset by retrieving new PDB structures and parsing individual chains into graphs, improving diversity and reducing noise.
Experimental results: We report initial training outcomes (loss curves over 10 epochs) that demonstrate effective learning behavior.
Implications: We reflect on how this approach could scale to proteome-wide annotation and aid drug discovery by identifying functional sites.
Methods
Data Preparation and Preprocessing:
Our training data consisted of proteins with known functions (Gene Ontology annotations) and available 3D structures. We preprocessed data using the following steps:
Automated structure retrieval: We periodically queried the RCSB PDB repository to obtain new or updated entries. For each PDB ID, we downloaded the mmCIF file. Using the Biopython PDB module, we parsed the file into a Structure object, which conveniently handled atomic coordinates and chain segmentation (Cock et al., 2009).
Chain-specific parsing: Multi-chain PDB entries were split, with each chain treated as a separate protein instance. Using the Structure object, we iterated over chains and extracted only the coordinates of that chain’s residues. This process isolated the geometry and sequence of each polypeptide, avoiding the mixing of interfaces (unless interface information was desired).
Residue graph construction: For each chain, we built an undirected graph where each node corresponded to a residue (typically represented by the Cα atom or centroid). We then added an edge (i, j) if the distance between residues i and j fell below a threshold of 10 angstroms, indicating a contact in 3D space. This yielded an adjacency matrix. This residue-level graph encoded the protein’s folded geometry.
Sequence embedding extraction: We obtained the amino acid sequence of the chain using the Hugging Face library, and we loaded the pre-trained ProtBERT model (Rostlab’s version), which provided 768-dimensional embeddings for each residue token (Brandes et al., 2022; Rives et al., 2021). The raw sequence was tokenized and fed into ProtBERT; we used the last hidden state as the sequence feature tensor. These embeddings summarized context over the entire protein, incorporating information learned from hundreds of millions of sequences.
Function labels (GO terms): For supervised learning, each protein chain was associated with a set of GO annotations (molecular function, biological process, etc.) obtained from curated databases. We treated function prediction as a multi-label classification: the model output a probability for each of the GO terms. During training, we used the binary cross-entropy loss across terms.
Model Architecture
Our network had two main branches that processed the protein sequence and structure separately, and then merged:
Sequence branch (ProtBERT): We embedded the raw amino acid sequence with the pre-trained ProtBERT model. Formally, given input tokens, the model outputs embeddings. We froze ProtBERT weights so they served as a static feature extractor, reducing computational cost and overfitting. These embeddings were then passed through a linear layer to project to the GCN input dimension.
Structure branch (GCN): We constructed a GCN operating on the residue contact graph. Each node, i, initially carried the ProtBERT embedding. We applied graph convolutional layers according to the method proposed by Kipf–Welling (Kipf & Welling, 2016). Each layer computes:
H⁽ˡ⁺¹⁾ = ReLU( D̃⁻¹ᐟ² · Ã · D̃⁻¹ᐟ² · H⁽ˡ⁾ · W⁽ˡ⁾ )
Where:
	H⁽ˡ⁾: Node feature matrix at layer l
W⁽ˡ⁾: Trainable weight matrix at layer l
Ã = A + I: Adjacency matrix with added self-loops
D̃: Degree matrix of Ã
D̃⁻¹ᐟ² Ã D̃⁻¹ᐟ²: Symmetric normalization of the adjacency matrix
ReLU: Rectified Linear Unit activation function

After several layers, each node’s feature aggregated information from its spatial neighbors, capturing local 3D context.

Global pooling and output:
To create a representation of the entire protein, we applied global mean pooling over the final layer of node features. This produced a fixed-size vector regardless of the protein sequence length. If the output of the last GCN layer was H^(L_g) ∈ ℝ^(L × h), where L is the number of residues and h is the hidden size, then the mean-pooled vector was:

This vector could be optionally concatenated with global sequence-level features. The resulting vector was passed through one or more fully connected (dense) layers with ReLU activations.
The final layer outputs K logits, each representing a GO term. These logits were passed through a sigmoid function to produce multi-label probabilities. In practice, we sometimes used more GCN layers or hidden units. We also experimented with variants such as Graph Attention (GAT) and multi-graph channels, but the above architecture sufficed for our initial tests. Importantly, by sharing the ProtBERT embeddings in both branches (i.e., using the same residue features for sequence and structure processing), we ensured that the network jointly refined how sequence context and spatial context contributed to function. This two-branch design was inspired by DeepFRI’s two-stage model (Gligorijević et al., 2021), although we used a transformer-based PLM and an explicit graph pooling layer.
Training pipeline:
We trained the network on proteins with known GO annotations in a supervised multi-label setting. Key training details included:
Loss function: We used multi-label binary cross-entropy (BCE) across all Gene Ontology (GO) terms. This loss function was appropriate because each protein could be associated with multiple GO terms independently.
Optimizer: We employed the Adam optimizer. Only the GCN and classification weights were learned.
Batching: Graph inputs were batched using PyTorch Geometric, which handled varying graph sizes. Since proteins had different lengths, we padded sequences to the same token length in each batch for ProtBERT.
Epochs and early stopping: We trained for approximately 10–20 epochs, monitoring training and validation loss. Early stopping was applied if validation loss plateaued. In our initial experiments, 10 epochs yielded substantial loss reduction. We also experimented with dropout (0.1–0.2) in the fully connected layers to prevent overfitting.
Data splits: The dataset was split purely at the protein level, ensuring no overlapping sequences. We also ensured that proteins in different splits shared no high sequence identity (e.g., <30%) to test generalization (Radivojac et al., 2013).
Implementation: Our code was implemented in PyTorch with PyTorch Geometric. Preprocessing (PDB parsing, graph creation) was performed in Python using Biopython (Cock et al., 2009). Sequence tokenization and ProtBERT inference used Hugging Face’s Transformers library.
Benchmarks: To benchmark performance, we tracked the model’s loss through 50 epochs and compared the results.
Results
We evaluated the effectiveness of our models using the binary cross-entropy loss across epochs. Two primary configurations were tested: a standard Graph Convolutional Network (GCN) trained solely on protein structure data, and a hybrid GCN + PLM model that incorporates both structural and sequence-level information via protein language model embeddings.
The plain GCN model, trained using only node embeddings derived from ProtBert token IDs and structural connectivity from C-alpha distances, showed a steady and prolonged convergence pattern. Starting with an initial loss of 0.6929 at epoch 1, the model gradually reduced the loss over time, reaching 0.5744 by epoch 10 and 0.0911 by epoch 20. The convergence continued into deeper training, with losses dropping to 0.0131 at epoch 25 and 0.0007 by epoch 50. This slow but consistent decline in loss suggests the GCN was capable of learning useful structural patterns, though it required many epochs to reach optimal performance.

By contrast, the GCN + PLM model, which fused structural data with contextualized embeddings from ProtBert, demonstrated a far more rapid convergence. The model began with a loss of 0.6673 at epoch 1 and reached 0.1725 by just epoch 3. Remarkably, by epoch 5, the loss dropped to 0.0005, and by epoch 10, it stabilized at 0.0001, indicating that the model had already achieved near-optimal classification performance. This dramatic improvement in both speed and accuracy highlights the significant value of incorporating protein language models into the representation learning pipeline.
These findings clearly show that the integration of pre-trained language model embeddings not only accelerates convergence but also enhances the predictive capability of the model. The GCN + PLM approach learns rich protein representations in far fewer epochs and achieves significantly lower loss values, underscoring the benefits of multimodal input fusion in protein function prediction (Yang et al., 2024).
Discussion
Our integrated GCN+PLM approach effectively combines sequence and structure information. The experimental trend of steady loss decline suggests the network successfully exploits both sources. DeepFRI’s results similarly showed that jointly using graph convolutions and language-model features outperforms using either alone or with an LSTM (Gligorijević et al., 2021)￼.  Intuitively, the PLM (ProtBERT) captures long-range sequence patterns that may hint at function (e.g., motifs conserved in enzyme families), while the GCN enforces that spatially proximate residues can jointly influence function (e.g., a catalytic triad).

In addition, our architecture is flexible. For example, one could replace ProtBERT with other PLMs (ESM, ProteinBERT, or even fine-tuned variants) to use for more specific types of proteins. Our work demonstrates proof-of-principle, but more advanced GNN layers (graph attention, relational graphs) could be incorporated (Xu et al., 2019), increasing the efficiency of the model.

The proposed data expansion pipeline is a key future direction. By continuously retrieving new PDBs, splitting chains, and incorporating even predicted structures such as AlphaFold DB (Jumper et al., 2021), we can greatly enlarge the training set.  Expanding the quantity in which we train the model should improve generalization: training on a few thousand proteins risks bias, whereas including tens or hundreds of thousands would cover more of the sequence-structure space.  DeepFRI authors similarly augmented their training with homology models to boost function coverage ￼(Gligorijević et al., 2021).  Chain-specific parsing increases sample count and ensures that each polypeptide’s function is learned in context (e.g., avoiding cross-talk from multi-chain complexes).

One challenge in this pipeline is label noise: not all PDB chains have complete or accurate GO annotations.  We may use thresholding or label cleaning, by excluding very general GO terms, to mitigate this.  As the dataset grows, techniques like curriculum learning or self-training, where high-confidence predictions are added as pseudo-labels, could further improve robustness.

Beyond GO terms, our framework could be adapted to related tasks. The residue-level interpretability has the potential to highlight drug-binding sites or allosteric regions, aiding drug discovery efforts.  Moreover, since ProtBERT and similar models can process extremely long sequences, our method is capable of handling large proteins or multi-domain chains, unlike older LSTM-based approaches, allowing it to decipher a larger array of naturally occurring proteins.

Limitations

Our current implementation is computationally heavy due to ProtBERT inference. In production, one could use distilled or smaller PLMs (Brandes et al., 2022)￼ or freeze embeddings as we did.  Also, 3D structure input means the method is limited by available models. However, with resources like AlphaFold DB providing predicted structures for nearly all sequences, this constraint is loosening (Jumper et al., 2021).  Our automated pipeline could seamlessly ingest AlphaFold predictions in addition to experimental PDB entries.
Conclusion

We have presented a comprehensive framework for functional annotation of proteins by integrating graph convolutional neural networks with protein language model embeddings.  Through a detailed description of the data preprocessing, architecture, and training pipeline, we demonstrate how to construct such a hybrid model in practice. Our initial experiments show promising learning behavior, and we expect that scaling up data via automated PDB retrieval and chain parsing will further enhance performance.

This work has significant implications.  A scalable, accurate function predictor can accelerate large-scale genome and metagenome annotation, filling gaps left by homology methods. By pinpointing functional residues via integrated sequence-structure signals, the model could guide mutagenesis experiments or identify druggable sites. In drug discovery, annotating novel proteins (e.g., microbial enzymes or orphan GPCRs) can suggest new therapeutic targets or enzymes for bioengineering.
References
Brandes, N., Ofer, D., Peleg, Y., Rappoport, N., & Linial, M. (2022). ProteinBERT: A universal deep-learning model of protein sequence and function. Bioinformatics, 38(8), 2102–2110. https://doi.org/10.1093/bioinformatics/btac061
Cock, P. J. A., Antao, T., Chang, J. T., Chapman, B. A., Cox, C. J., Dalke, A., Friedberg, I., Hamelryck, T., Kauff, F., Wilczynski, B., & de Hoon, M. J. L. (2009). Biopython: Freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics, 25(11), 1422–1423. https://doi.org/10.1093/bioinformatics/btp163
Gligorijević, V., Renfrew, P. D., Kosciolek, T., Leman, J. K., Berenberg, D., Vatanen, T., Chandler, C., Taylor, B. C., Fisk, I. M., Vlamakis, H., Xavier, R. J., Knight, R., Cho, K., Bonneau, R., & Sander, C. (2021). Structure-based protein function prediction using graph convolutional networks. Nature Communications, 12, 3168. https://doi.org/10.1038/s41467-021-23303-9
Jiang, Y., Oron, T. R., Clark, W. T., Bankapur, A. R., D'Andrea, D., Lepore, R., Funk, C. S., Kahanda, I., Verspoor, K. M., Ben-Hur, A., Koo, D. C. E., Penfold-Brown, D., Shasha, D., Youngs, N., Bonneau, R., Linial, M., Radivojac, P., Friedberg, I., & Mooney, S. D. (2016). An expanded evaluation of protein function prediction methods shows an improvement in accuracy. Genome Biology, 17, 184. https://doi.org/10.1186/s13059-016-1037-6
Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., Žídek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A. J., Cowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., ... Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873), 583–589. https://doi.org/10.1038/s41586-021-03819-2
Kipf, T. N., & Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv. https://arxiv.org/abs/1609.02907
Radivojac, P., Clark, W. T., Oron, T. R., Schnoes, A. M., Wittkop, T., Sokolov, A., Graim, K., Funk, C., Verspoor, K., Ben-Hur, A., Pandey, G., Yunes, J. M., Talwalkar, A., Repo, S., Jones, D. T., Friedberg, I., Bonneau, R., Bradley, P., Rex, D. K., ... Mooney, S. D. (2013). A large-scale evaluation of computational protein function prediction. Nature Methods, 10, 221–227. https://doi.org/10.1038/nmeth.2340
Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., & Fergus, R. (2021). Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15), e2016239118. https://doi.org/10.1073/pnas.2016239118
Xu, K., Hu, W., Leskovec, J., & Jegelka, S. (2019). How powerful are graph neural networks? arXiv. https://arxiv.org/abs/1810.00826
Yang, Y., Zhu, Q., Zhang, L., Li, M., Wang, J., & Peng, J. (2024). Protein function prediction with multitask deep neural networks. Nature Communications, 15, 1113. https://doi.org/10.1038/s41467-024-45089-3




